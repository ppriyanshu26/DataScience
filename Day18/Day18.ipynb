{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e2f67e2",
   "metadata": {},
   "source": [
    "# üéØ Bias-Variance Trade-Off in Machine Learning\n",
    "\n",
    "- Understanding the `**bias-variance trade-off**` is crucial for building models that generalize well to unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç What is Bias?\n",
    "- **Bias** refers to errors due to overly simplistic assumptions in the learning algorithm.\n",
    "- High bias can cause the model to **miss relevant patterns** in the data (üìâ underfitting).\n",
    "- Common in models like **linear regression** or shallow decision trees when applied to complex problems.\n",
    "\n",
    "### üî¥ Signs of High Bias:\n",
    "- Poor performance on training data.\n",
    "- Poor performance on test data.\n",
    "- Model is **too simple** to capture the complexity of the data.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ What is Variance?\n",
    "- **Variance** refers to the model‚Äôs sensitivity to fluctuations in the training data.\n",
    "- High variance can cause the model to **learn noise** in the data (üìà overfitting).\n",
    "- Common in models like **deep decision trees**, **polynomial regression**, or complex neural networks without regularization.\n",
    "\n",
    "### üî¥ Signs of High Variance:\n",
    "- Excellent performance on training data.\n",
    "- Poor performance on test/validation data.\n",
    "- Model is **too complex** and over-adapts to training data.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è Why a Balanced Model is Essential\n",
    "- A balanced model achieves the **right trade-off**:\n",
    "  - Low enough bias to capture data patterns\n",
    "  - Low enough variance to generalize to new data\n",
    "- üèÜ Goal: **Minimize total error (bias¬≤ + variance + irreducible error)**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ How to Avoid Overfitting (High Variance)\n",
    "\n",
    "1. üîΩ **Simplify the Model**: Avoid unnecessarily deep or complex models.\n",
    "2. üõ†Ô∏è **Regularization**: Use L1 (Lasso) or L2 (Ridge) regularization to penalize overly complex models.\n",
    "3. üîÅ **Cross-Validation**: Use techniques like K-Fold CV to ensure your model performs well on unseen data.\n",
    "4. ‚èπÔ∏è **Early Stopping**: Stop training once performance on validation data begins to degrade.\n",
    "5. üìä **Use More Training Data**: More examples help the model generalize better.\n",
    "6. üì¶ **Data Augmentation** (for images/text): Increase training set variety artificially.\n",
    "7. üßπ **Feature Selection**: Remove irrelevant or noisy features to prevent overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† How to Avoid Underfitting (High Bias)\n",
    "\n",
    "1. ‚ûï **Use a more complex model**: Try moving from linear models to tree-based or neural networks.\n",
    "2. üßÆ **Add more features**: Include relevant predictors that can help the model learn better.\n",
    "3. üõ†Ô∏è **Reduce regularization** if it's too strong.\n",
    "4. üîÑ **Allow more training time** or train for more epochs (if early stopping is applied too early).\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Summary Table\n",
    "- ‚ú® **Pro Tip**: Always visualize training vs. validation loss curves to detect overfitting or underfitting patterns early!\n",
    "\n",
    "    | Issue         | Cause                       | Effect                           | Fixes                                  |\n",
    "    |---------------|-----------------------------|-----------------------------------|----------------------------------------|\n",
    "    | High Bias     | Model too simple             | Underfitting                      | Use more complex model, add features   |\n",
    "    | High Variance | Model too complex            | Overfitting                       | Regularization, early stopping, CV     |\n",
    "    | Balanced      | Optimal bias and variance    | Good generalization               | Careful tuning and model validation    |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfeb5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997617ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd3cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "\n",
    "df = pd.DataFrame(data=x_pca, columns=['PC 1','PC 2'])\n",
    "df['Target'] = Y\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4edd286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance\n",
    "\n",
    "variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(variance)\n",
    "\n",
    "print(f\"Explained Variance Ratio: {variance}\\nCumulative Variance: {cumulative_variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff47fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(variance) + 1), variance, 'o-', color='blue', label='Individual Explained Variance')\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 's--', color='orange', label='Cumulative Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xticks(range(1, len(variance) + 1))\n",
    "plt.legend(loc='center right')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e39915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "df = pd.read_csv('customer_data.csv')\n",
    "\n",
    "df.fillna({'fea_2':df['fea_2'].mean()}, inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df = df.drop(['id'], axis=1)\n",
    "\n",
    "X = df.drop(['label'], axis=1)\n",
    "Y = df['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4dfc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f22da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "x_pca = pca.fit_transform(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf9a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "\n",
    "df = pd.DataFrame(data=x_pca, columns=['PC 1','PC 2', 'PC 3'])\n",
    "df['Target'] = Y\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98db95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance\n",
    "\n",
    "variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(variance)\n",
    "\n",
    "print(f\"Explained Variance Ratio: {variance}\\nCumulative Variance: {cumulative_variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f34df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(variance) + 1), variance, 'o-', color='blue', label='Individual Explained Variance')\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 's--', color='orange', label='Cumulative Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xticks(range(1, len(variance) + 1))\n",
    "plt.legend(loc='center right')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
